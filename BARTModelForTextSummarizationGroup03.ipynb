{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AXjQj_gUOt6"
      },
      "outputs": [],
      "source": [
        "# Installation of packages\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "!pip install datasets\n",
        "\n",
        "!pip install rouge\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Packages\n",
        "from transformers import pipeline, BartForConditionalGeneration, BartTokenizer, TrainingArguments, Trainer\n",
        "from datasets import list_datasets, load_dataset\n",
        "from rouge import Rouge\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import datasets\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import BartTokenizer\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, TrainingArguments, Trainer\n",
        "from transformers import BartConfig, BartForConditionalGeneration\n",
        "from torch.nn import ReLU, GELU\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import datasets\n",
        "import torch\n",
        "from torch.nn import ELU\n",
        "import torch.nn.functional as F\n",
        "from transformers import AdamW\n"
      ],
      "metadata": {
        "id": "aLFY4E6jUlLT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "dataset = load_dataset('cnn_dailymail', \"3.0.0\")\n",
        "rouge = Rouge(metrics=['rouge-1','rouge-2','rouge-l'])\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "EoXWo9G9WbJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and the tokenizer\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "tzUt2f-8WoCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the rouge score for Pre - Trained Model\n",
        "\n",
        "generated_summaries,reference_summaries = [],[]\n",
        "# Loop over the dataset and generate summaries\n",
        "for example in dataset[\"train\"]:\n",
        "    # Tokenize the input text and truncate it to a maximum length of 1024 tokens\n",
        "    input_ids = tokenizer.encode(example[\"article\"], max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "    # Generate the summary\n",
        "    input_ids = input_ids.to(device)\n",
        "    summary_ids = model.generate(input_ids, num_beams=4, length_penalty=2.0, max_length=142, min_length=56, no_repeat_ngram_size=3)\n",
        "    summary_ids = summary_ids.to('cpu')\n",
        "    summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "    generated_summaries.append(summary)\n",
        "    reference_summaries.append(example[\"highlights\"])\n",
        "\n",
        "# Calculate ROUGE scores for each summary pair\n",
        "scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n",
        "\n",
        "# Print the ROUGE scores and their averages for the dataset\n",
        "for metric, values in scores.items():\n",
        "    print(f\"ROUGE-{metric} scores:\\n\")\n",
        "    print(values)\n",
        "    print(f\"Average ROUGE-{metric} score: {values['f']}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "d2AOXtDzW2xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing Compact Ratio and adjust min_length, max_length and length penalty hyperparameters\n",
        "\n",
        "generated_summaries, reference_summaries = [], []\n",
        "# Loop over the dataset and generate summaries\n",
        "print(\"A len \\t\\t ES len \\t\\t AS len \\t\\t E CP \\t\\t A CP\")\n",
        "for example in dataset[\"test\"]:\n",
        "    # Tokenize the input text and truncate it to a maximum length of 1024 tokens\n",
        "    input_ids = tokenizer.encode(example[\"article\"], max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate the summary\n",
        "    input_ids = input_ids.to(device)\n",
        "    lenInput = len(example[\"article\"])\n",
        "    min_length = int((0.05*lenInput))\n",
        "    max_length = int((0.10*lenInput))\n",
        "\n",
        "    summary_ids = model.generate(input_ids, num_beams=4, length_penalty=3, min_length=min_length, max_length=max_length, no_repeat_ngram_size=3)\n",
        "    summary_ids = summary_ids.to('cpu')\n",
        "    summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "    generated_summaries.append(summary)\n",
        "    reference_summaries.append(example[\"highlights\"])\n",
        "    lenA = len(example[\"article\"])\n",
        "    lenES = len(example[\"highlights\"])\n",
        "    lenAS = len(summary)\n",
        "    ECP = (lenA-lenES)*100/lenA\n",
        "    ECP = str(round(ECP, 2))\n",
        "    ACP = (lenA-lenAS)*100/lenA\n",
        "    ACP = str(round(ACP, 2))\n",
        "    print(lenA, \"\\t\\t\", lenES, \"\\t\\t\\t\", lenAS ,\"\\t\\t\\t\", ECP, \"\\t\\t\", ACP) \n",
        "\n",
        "scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n",
        "\n",
        "# Print the ROUGE scores and their averages for the dataset\n",
        "for metric, values in scores.items():\n",
        "    print(f\"\\nROUGE-{metric} scores: \\n\")\n",
        "    print(values)\n",
        "    print(f\"Average ROUGE-{metric} score: {values['f']}\\n\")"
      ],
      "metadata": {
        "id": "U0lDwpb9XZOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BART model\n",
        "bart_config = BartConfig.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "# Set the activation function of the decoder's first layer to ReLU\n",
        "# bart_config.activation_function = 'relu'\n",
        "# bart_config.decoder_layerdrop = 0.1\n",
        "# bart_config.dropout = 0.1\n",
        "# bart_config.activation_dropout = 0.0\n",
        "# bart_config.decoder_attention_heads = 30\n",
        "# bart_config.decoder_layers = 20\n",
        "model.config = bart_config\n",
        "\n",
        "\n",
        "# Set the activation function of the decoder's first layer to SiLU\n",
        "#model.model.decoder.layers[0].activation_fn = ReLU()"
      ],
      "metadata": {
        "id": "TBXRWNUeX2x4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing Activation function to RELU (It puts all negative values as zero and remaining values as it is)\n",
        "bart_config = BartConfig.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_config.activation_function = 'relu'\n",
        "model.config = bart_config"
      ],
      "metadata": {
        "id": "4rTCFtLoX_j2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing Activation function to RELU (It puts all negative values as zero and remaining values as it is)\n",
        "bart_config = BartConfig.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_config.activation_function = 'elu'\n",
        "model.config = bart_config\n",
        "for layer in model.model.decoder.layers:\n",
        "    layer.activation_fn = ELU()"
      ],
      "metadata": {
        "id": "uZ6XJgm3YigM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing Activation function to mish \n",
        "class Mish(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))\n",
        "        \n",
        "bart_config = BartConfig.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_config.activation_function = 'mish'\n",
        "model.config = bart_config\n",
        "\n",
        "for layer in model.model.decoder.layers:\n",
        "    layer.activation_fn = Mish()"
      ],
      "metadata": {
        "id": "pe9u5fe6ZKKa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing Attention heads\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Change the number of attention heads in the encoder and decoder layers to 30\n",
        "bart_config = model.config\n",
        "bart_config.encoder_attention_heads = 30\n",
        "bart_config.decoder_attention_heads = 30\n",
        "\n",
        "# Set the updated configuration to the model\n",
        "model.config = bart_config"
      ],
      "metadata": {
        "id": "Hjz0Hs4GaBgy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing attention heads to 50\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Change the number of attention heads in the encoder and decoder layers to 30\n",
        "bart_config = model.config\n",
        "bart_config.encoder_attention_heads = 50\n",
        "bart_config.decoder_attention_heads = 50\n",
        "\n",
        "# Set the updated configuration to the model\n",
        "model.config = bart_config"
      ],
      "metadata": {
        "id": "DNyN6tGNbCFt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Change the number of encoder and decoder layers\n",
        "bart_config = model.config\n",
        "bart_config.encoder_layers = 20  # Change the number of encoder layers\n",
        "bart_config.decoder_layers = 20  # Change the number of decoder layers\n",
        "\n",
        "# Set the updated configuration to the model\n",
        "model.config = bart_config"
      ],
      "metadata": {
        "id": "5TxuND7qb6bC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Create the optimizer and set its parameters\n",
        "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "# Set the optimizer to the model\n",
        "model.optimizer = optimizer"
      ],
      "metadata": {
        "id": "tgymGrOjcftQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Create the optimizer and set its parameters\n",
        "optimizer = AdamW(model.parameters(), lr=1e-6, weight_decay=0.1)\n",
        "\n",
        "# Set the optimizer to the model\n",
        "model.optimizer = optimizer"
      ],
      "metadata": {
        "id": "htThzB7KdKbq"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}