{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O11K161MscgS"
      },
      "outputs": [],
      "source": [
        "!pip install simplet5\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9JmZQq0symL"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install simpletransformers\n",
        "!pip install rouge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT7um3d1tQHh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeI9IN2oxvOh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBqMsmr-tSKm"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
        "\n",
        "valid_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation\")\n",
        "\n",
        "test_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjrZ4cWmv0Bu"
      },
      "outputs": [],
      "source": [
        "train_data = pd.DataFrame(train_dataset.to_dict())\n",
        "test_data = pd.DataFrame(test_dataset.to_dict())\n",
        "validate_data = pd.DataFrame(valid_dataset.to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape[0]"
      ],
      "metadata": {
        "id": "hKE7cBv1MnHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ddb036c-8894-43e9-a2b8-68ce9f13b6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5742"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape[0]"
      ],
      "metadata": {
        "id": "CFIsCB-mORw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1121eeab-e463-4501-80df-62111be54411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "230"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate_data.shape[0]"
      ],
      "metadata": {
        "id": "K7-uEE4xORVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e2fafb-04f2-420c-e9ab-8ab3f64ec18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "267"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B94faIney76r"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(article):\n",
        "    # Convert text to lowercase\n",
        "    article = article.lower()\n",
        "    \n",
        "    # Remove unwanted characters\n",
        "    article = re.sub(r'\\b\\d+\\b', '', article)\n",
        "    article = re.sub(r'[^\\w\\s]', '', article)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    article = ' '.join([word for word in article.split() if word not in stop_words])\n",
        "    \n",
        "    # Remove punctuation marks\n",
        "    article = article.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    return article\n",
        "\n",
        "train_data['article'] = train_data['article'].apply(clean_text)\n",
        "\n",
        "train_data['highlights'] = train_data['highlights'].apply(clean_text)\n",
        "\n",
        "train_data['article']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqd02F3yA_qT"
      },
      "outputs": [],
      "source": [
        "test_data['article'] = test_data['article'].apply(clean_text)\n",
        "\n",
        "test_data['highlights'] = test_data['highlights'].apply(clean_text)\n",
        "\n",
        "test_data['article']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate_data['article'] = validate_data['article'].apply(clean_text)\n",
        "validate_data['highlights'] = validate_data['highlights'].apply(clean_text)\n",
        "validate_data['article']"
      ],
      "metadata": {
        "id": "5VTop-UXWs1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMprC6h8nG8T"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.rename(columns={\"highlights\":\"target_text\", \"article\":\"input_text\"})\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa-DA0OeBPoa"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.rename(columns={\"highlights\":\"target_text\", \"article\":\"input_text\"})\n",
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate_data = validate_data.rename(columns={\"highlights\":\"target_text\", \"article\":\"input_text\"})\n",
        "validate_data"
      ],
      "metadata": {
        "id": "cA3Z7b8vXH53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0VPQeqtppLj"
      },
      "outputs": [],
      "source": [
        "train_data = train_data[['input_text', 'target_text']]\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPFSBbOwBY6n"
      },
      "outputs": [],
      "source": [
        "test_data = test_data[['input_text', 'target_text']]\n",
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate_data = validate_data[['input_text', 'target_text']]\n",
        "validate_data"
      ],
      "metadata": {
        "id": "MqTuDuFwYcUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJuKKHvXpseZ"
      },
      "outputs": [],
      "source": [
        "train_data['input_text'] = \"summarize: \" + train_data['input_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pMv3mTwp4LM"
      },
      "outputs": [],
      "source": [
        "train_data[\"prefix\"] = \"summarize\"\n",
        "\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btXfnmdj0d16"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdU3df7z0gjW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json \n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "from simpletransformers.t5 import T5Model\n",
        "import sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# load the T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "device = torch.device('cuda')\n",
        "model.to('cuda')\n",
        "data = pd.DataFrame()\n",
        "\n",
        "def generate_summary(text):\n",
        "    # preprocess the text\n",
        "    preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "    t5_prepared_Text = \"summarize: \" + preprocess_text\n",
        "\n",
        "    # encode the text using the tokenizer\n",
        "    tokenized_text = tokenizer.encode(t5_prepared_Text, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    tokenized_text.to('cuda')\n",
        "    # generate the summary\n",
        "    summary_ids = model.generate(tokenized_text,\n",
        "                                    num_beams=4,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    min_length=30,\n",
        "                                    max_length=250,\n",
        "                                    early_stopping=True)\n",
        "    summary_ids.to('cpu')\n",
        "    # decode the summary and return\n",
        "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return output\n",
        "\n",
        "# apply the function to the dataframe and save the summaries\n",
        "data['summary'] = test_data['input_text'].apply(generate_summary)\n",
        "data.to_csv('output_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "OQjlBnpY3kOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STSsb1i0GSrq"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "expected = test_data['target_text'].tolist()\n",
        "generated = pd.read_csv('/content/output_data.csv')['summary'].tolist()\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "scores = rouge.get_scores(generated, expected, avg=True)\n",
        "\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"reprocess_input_data\": False,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"max_seq_length\": 512,\n",
        "    \"num_train_epochs\": 5,\n",
        "    \"num_beams\": None, \n",
        "    \"do_sample\": True,\n",
        "    \"top_k\": 10,\n",
        "    \"top_p\": 0.5,\n",
        "    \"use_multiprocessing\": False,\n",
        "    \"save_steps\": -1,\n",
        "    \"save_eval_checkpoints\": True,\n",
        "    \"evaluate_during_training\": False,\n",
        "    \"adam_epsilon\": 1e-08,\n",
        "    \"eval_batch_size\": 10,\n",
        "    \"fp_16\": False,\n",
        "    \"gradient_accumulation_steps\": 16,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"n_gpu\": 1,\n",
        "    \"seed\": 42,\n",
        "    \"train_batch_size\": 10,\n",
        "    \"warmup_steps\": 0,\n",
        "    \"weight_decay\": 0.01,\n",
        "}"
      ],
      "metadata": {
        "id": "yLRNF7PSIjLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obKMZ4QhIIrb"
      },
      "outputs": [],
      "source": [
        "model = T5Model(\"t5\",\"t5-small\", args=args, use_cuda=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = model.train_model(train_data, eval_data=test_data, use_cuda=True, acc=sklearn.metrics.accuracy_score)"
      ],
      "metadata": {
        "id": "BTtYQSgOeBcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "def generate_summary(text):\n",
        "    # preprocess the text\n",
        "    preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "    t5_prepared_Text = \"summarize: \" + preprocess_text\n",
        "\n",
        "    # encode the text using the tokenizer\n",
        "    tokenized_text = tokenizer.encode(t5_prepared_Text, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    tokenized_text.to('cuda')\n",
        "    # generate the summary\n",
        "    summary_ids = train.generate(tokenized_text,\n",
        "                                    num_beams=4,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    min_length=30,\n",
        "                                    max_length=250,\n",
        "                                    early_stopping=True)\n",
        "    summary_ids.to('cpu')\n",
        "    # decode the summary and return\n",
        "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return output\n",
        "# apply the function to the dataframe and save the summaries\n",
        "data = pd.DataFrame()\n",
        "for i in test_data['input_text']:\n",
        "  \n",
        "  data['summary']= i.apply(generate_summary)\n",
        "  \n",
        "# data['summary'] = test_data['input_text'].apply(generate_summary)\n",
        "data.to_csv('output.csv', index=False)"
      ],
      "metadata": {
        "id": "vmqabrlpShiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root_dir = os.getcwd()\n",
        "trained_model_path = os.path.join(root_dir,\"outputs\")\n",
        "args = {\n",
        "    \"max_seq_length\": 512,\n",
        "    \"num_return_sequences\": 5,\n",
        "    \"min_length\": 30,\n",
        "    \"max_length\": 250,\n",
        "    \"early_stopping\": True,\n",
        "    \"repetition_penalty\": 1.5, \n",
        "    \"length_penalty\": 2.0,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.5,\n",
        "}\n",
        "trained_model = T5Model(\"t5\",trained_model_path,args=args)\n"
      ],
      "metadata": {
        "id": "9GEv7xvBWTpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict"
      ],
      "metadata": {
        "id": "MVECcryFdknI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the text data from the 'text' column\n",
        "input_data = list(test_data['input_text'])\n",
        "\n",
        "# Pass the input data to the model's predict() method\n",
        "predictions = trained_model.predict(input_data)"
      ],
      "metadata": {
        "id": "nG9EyQipX6BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = pd.DataFrame()\n",
        "prediction['summary'] = predictions\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "prediction.to_csv('tested_output.csv', index=False)"
      ],
      "metadata": {
        "id": "QRvtuKnQe8b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "expected = test_data['target_text'].tolist()\n",
        "generated = pd.read_csv('/content/tested_output.csv')['summary'].tolist()\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "scores = rouge.get_scores(generated, expected, avg=True)\n",
        "\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "SG_qZE30g8gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''summarize: Fit-again Betsen in France squad France have brought flanker Serge Betsen back into their squad to face England at Twickenham on Sunday.But the player, who missed the victory over Scotland through injury, must attend a disciplinary hearing on Wednesday after being cited by Wasps. Serge has a good case so we are confident he will play said France coach Bernard Laporte. The inexperienced Nicolas Mas, Jimmy Marlu and Jean-Philippe Grandclaude are also included in a 22-man squad. The trio have been called up after Pieter de Villiers, Ludovic Valbon and Aurelien Rougerie all picked up injuries in France's 16-9 win on Saturday.Laporte said he was confident that Betsen would be cleared by the panel investigating his alleged trip that broke Wasps centre Stuart Abbott's leg. If he was to be suspended, we would call up Imanol Harinordoquy or Thomas Lievremont,said Laporte, who has dropped Patrick Tabacco. We missed Serge badly against Scotland. He has now recovered from his thigh injury and played on Saturday with Biarritz.France's regular back-row combination of Betsen, Harinordoquy and Olivier Magne were all missing from France side at the weekend because of injury. Laporte is expected to announce France starting line-up on Wednesday.Forwards: Nicolas Mas, Sylvain Marconnet, Olivier Milloud, William Servat, Sebastien Bruno, Fabien Pelous, Jerome Thion, Gregory Lamboley, Serge Betsen, Julien Bonnaire, Sebastien Chabal, Yannick Nyanga. Backs: Dimitri Yachvili, Pierre Mignoni, Frederic Michalak, Yann Delaigue, Damien Traille, Brian Liebenberg, Jean-Philippe Grandclaude, Christophe Dominici, Jimmy Marlu, Pepito Elhorga\n",
        " '''\n",
        "predict = trained_model.predict([text])"
      ],
      "metadata": {
        "id": "zpc5i_fZdNyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"summarize\"\n",
        "\n",
        "pred = trained_model.predict([f\"{prefix}: Fit-again Betsen in France squad France have brought flanker Serge Betsen back into their squad to face England at Twickenham on Sunday.But the player, who missed the victory over Scotland through injury, must attend a disciplinary hearing on Wednesday after being cited by Wasps. Serge has a good case so we are confident he will play said France coach Bernard Laporte. The inexperienced Nicolas Mas, Jimmy Marlu and Jean-Philippe Grandclaude are also included in a 22-man squad. The trio have been called up after Pieter de Villiers, Ludovic Valbon and Aurelien Rougerie all picked up injuries in France's 16-9 win on Saturday.Laporte said he was confident that Betsen would be cleared by the panel investigating his alleged trip that broke Wasps centre Stuart Abbott's leg. If he was to be suspended, we would call up Imanol Harinordoquy or Thomas Lievremont,said Laporte, who has dropped Patrick Tabacco. We missed Serge badly against Scotland. He has now recovered from his thigh injury and played on Saturday with Biarritz.France's regular back-row combination of Betsen, Harinordoquy and Olivier Magne were all missing from France side at the weekend because of injury. Laporte is expected to announce France starting line-up on Wednesday.Forwards: Nicolas Mas, Sylvain Marconnet, Olivier Milloud, William Servat, Sebastien Bruno, Fabien Pelous, Jerome Thion, Gregory Lamboley, Serge Betsen, Julien Bonnaire, Sebastien Chabal, Yannick Nyanga. Backs: Dimitri Yachvili, Pierre Mignoni, Frederic Michalak, Yann Delaigue, Damien Traille, Brian Liebenberg, Jean-Philippe Grandclaude, Christophe Dominici, Jimmy Marlu, Pepito Elhorga\"])\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "n272fBmnGZQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}